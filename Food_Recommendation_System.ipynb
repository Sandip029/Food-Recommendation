{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install langchain-groq -q\n",
        "!pip install langchain-core -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds2D0iQikwHH",
        "outputId": "6a4c471a-92c1-4375-ac64-7dd73c9c298b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/8.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m4.2/8.7 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.9/8.7 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m405.1/405.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRM9LhK0kbqG",
        "outputId": "c1d5b33b-64e0-4a90-ad4f-c858c6e2bb3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mood_food_prediction.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mood_food_prediction.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "# Set up the Streamlit page\n",
        "st.set_page_config(page_title=\"Food Suggestion System\", page_icon=\"沚ｴ\", layout=\"wide\")\n",
        "\n",
        "# Define the title and description\n",
        "st.title(\"沚ｴ Food Suggestion System\")\n",
        "st.markdown(\"\"\"\n",
        "    Welcome to the Food Suggestion System! 沍歃n",
        "    Enter your mood, current weather, and preference (veg or non-veg) below, and we'll recommend some delicious food for you.\n",
        "    Just fill out the fields and click 'Get Suggestion' to receive personalized recommendations!\n",
        "\"\"\")\n",
        "\n",
        "# Get the API key from environment variables\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "# Initialize the language model if API key is available\n",
        "if not GROQ_API_KEY:\n",
        "    st.error(\"API key is not set. Please configure the API key in the environment variables.\")\n",
        "else:\n",
        "    try:\n",
        "        # Initialize the language model with the API key\n",
        "        llm = ChatGroq(model=\"llama-3.1-70b-versatile\", api_key=GROQ_API_KEY)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to initialize the language model: {e}\")\n",
        "\n",
        "    # Define the Pydantic classes\n",
        "    class FoodSuggestion(BaseModel):\n",
        "        Food_Recommendation: str = Field(description=\"Suggested food based on mood, weather, and preference\")\n",
        "\n",
        "    class Response(BaseModel):\n",
        "        output: FoodSuggestion\n",
        "\n",
        "    # Function to display the response\n",
        "    def display_food_suggestion(response: Response):\n",
        "        if isinstance(response.output, FoodSuggestion):\n",
        "            suggestion = response.output.Food_Recommendation\n",
        "            st.write(f\"### Food Recommendation: {suggestion}\")\n",
        "        else:\n",
        "            st.error(\"Unknown response type received.\")\n",
        "\n",
        "    # Structure the model to return output based on the schema\n",
        "    try:\n",
        "        structured_llm = llm.with_structured_output(Response)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in structuring the language model output: {e}\")\n",
        "\n",
        "    # Create input form\n",
        "    with st.form(key='input_form'):\n",
        "        st.header(\"Please Enter Your Details\")\n",
        "        mood = st.text_input(\"Your Mood\", placeholder=\"e.g., happy, sad, excited\")\n",
        "        weather = st.text_input(\"Current Weather\", placeholder=\"e.g., sunny, cloudy, rainy\")\n",
        "        preference = st.selectbox(\"Food Preference\", [\"non-veg\", \"veg\"])\n",
        "        submit_button = st.form_submit_button(label=\"Get Suggestion\")\n",
        "\n",
        "    # Submit button action\n",
        "    if submit_button:\n",
        "        if not mood or not weather:\n",
        "            st.error(\"Please enter both mood and weather before submitting.\")\n",
        "        else:\n",
        "            try:\n",
        "                # Construct the input query\n",
        "                input_text = (f\"Suggest {preference} food for a person who is feeling {mood} \"\n",
        "                              f\"and the weather is {weather}.\")\n",
        "\n",
        "                # Invoke the model and display the result\n",
        "                response = structured_llm.invoke(input_text)\n",
        "                display_food_suggestion(response)\n",
        "            except Exception as e:\n",
        "                if 'tool_use_failed' in str(e):\n",
        "                    st.error(\"Please try to submit again. Ensure your input is clear and valid.\")\n",
        "                else:\n",
        "                    st.error(f\"Unable to process the query: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "\n",
        "ngrok.set_auth_token(\"2jhoNI1jfLmSpnrRlJ3hzjinWOz_6D3Wi4PDE4vVZrsYPRBY\")\n",
        "ngrok_tunnel = ngrok.connect(8501)\n",
        "print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"mood_food_prediction.py\"])\n",
        "\n",
        "import time\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Ngrok tunnel stopped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKuHPmzelgqp",
        "outputId": "d45df083-8936-4cc8-ebaf-7b682b699d8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://3a61-34-87-144-106.ngrok-free.app\n",
            "Ngrok tunnel stopped.\n"
          ]
        }
      ]
    }
  ]
}